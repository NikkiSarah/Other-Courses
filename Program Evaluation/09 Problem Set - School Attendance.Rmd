---
title: "Problem set 6: Mandatory school attendance program"
date: "1 September 2023"
output: 
  html_document: 
    toc: yes
  pdf_document: 
    latex_engine: xelatex
    toc: yes
  word_document: 
    toc: yes
editor_options: 
  chunk_output_type: console
---

---

There is substantial research and evidence that [class attendance has a positive and significant effect on student performance](http://graphics8.nytimes.com/packages/pdf/nyregion/20110617attendancereport.pdf). Because of this, state and local government agencies and school districts have designed programs and policies that incentivize students to not miss school days. Examples include tangible prizes like [colorful pendants and free tickets to events](https://www.nytimes.com/2011/06/17/nyregion/city-reduces-chronic-absenteeism-in-public-schools.html), [automated calls from celebrities](https://cityroom.blogs.nytimes.com/2011/02/10/schools-use-celebrity-robo-calls-to-battle-truancy/), or [class policies that mandate attendance](https://people.ucsc.edu/~cdobkin/Papers/2010%20Skipping%20class%20in%20college%20and%20exam%20performance%20Evidence%20from%20a%20regression%20discontinuity%20classroom%20experiment.pdf). 

Existing research has used a range of methods to test the relationship between attendance programs and student performance, including [simple regression analysis](https://dx.doi.org/10.1016/j.sbspro.2016.07.051), [randomized experiments](https://dx.doi.org/10.3200/JECE.39.3.213-227), and [regression discontinuity approaches](https://people.ucsc.edu/~cdobkin/Papers/2010%20Skipping%20class%20in%20college%20and%20exam%20performance%20Evidence%20from%20a%20regression%20discontinuity%20classroom%20experiment.pdf).

In this assignment, you will use regression discontinuity approaches to measure the effect of a hypothetical program on hypothetical student grades (this data is 100% fake). 

In this simulated program, high school students who have less than 80% attendance during their junior year (11th grade) are assigned to a mandatory school attendance program during their senior year (12th grade). This program requires them to attend school and also provides them with additional support and tutoring to help them attend and remain in school. At the end of their senior year, students take a final test to assess their overall learning in high school.

The dataset I've provided contains four columns:

- `id`: A randomly assigned student ID number
- `attendance`: The proportion of days of school attended during a student's junior year (ranges from 0 to 100)
- `treatment`: Binary variable indicating if a student was assigned to the attendance program during their senior year
- `grade`: A student's final test grade at the end of their senior year


```{r setup, warning=FALSE, message=FALSE}
library(tidyverse)
library(rdrobust)
library(rddensity)
library(broom)
library(modelsummary)

options(dplyr.summarise.inform = FALSE)

program <- read_csv("data/attendance_program.csv")
```


# Step 1: Determine if process of assigning treatment is rule-based

**Was assignment to this program based on an arbitrary rule? Is it a good candidate for a regression discontinuity approach? Why or why not?**

In order to join the school attendance program, students have had to record less than 80% of attendance in the previous year. Students with 80% or higher attendance aren't eligible for the program. Since there's a clear 80% rule, we can assume that the process of participating in the school attendance program is rule-based. As such, it appears to be a good candidate for a regression discontinuity approach.

# Step 2: Determine if the design is fuzzy or sharp

Make a plot that shows the running variable (`attendance`) on the x-axis and the program indicator variable (`treatment`) on the y-axis. Show the relationship using points (`geom_point`) and colour the points by `treatment`.

**How strict was the application of the rule? Did any students with attendance above 80% get into the attendance program, or did any students with attendance under 80% not get into the program? Is there a sharp difference in treatment at the cutpoint?**

```{r}
# Dot plot with attendance on the x-axis and treatment on the y-axis
ggplot(program, aes(x = attendance, y = treatment, colour = treatment)) +
  geom_point(size = 2.5, alpha = 0.5,
             position = position_jitter(width = 0, height = 0.25, seed = 1234)) +
  geom_vline(xintercept = 80) +
  labs(x = "Attendance", y = "Participated in attendance program") +
  guides(colour = "none") +
  theme_classic()
```

This looks pretty sharp - it doesn't look like students with over 80% attendance participated in the program. This is verified with a table. There are no students where `attendance` is at least 80 and `treatment` is true, and no students where `attendance` is less than 80 and `treatment` is true.

```{r}
program %>% 
  group_by(treatment, attendance >= 80) %>% 
  summarise(count = n())
```

This is a sharp design.

# Step 3: Check for discontinuity in running variable around cutpoint

Next, you should check that there was no manipulation in the running variable. We don't want to see a ton of students with 81% or 79% attendance, since that could be a sign that administrators fudged the numbers to either push students into the program or out of the program. 

First, make a histogram of the running variable and see if there are any big jumps around the threshold. Fill the histogram by `treatment` and add a vertical line at 80 to show the cut-off. Use an appropriate bin width. If the column near 80 is split into two different colours (it might be, since it might be showing 79 and 80 together), add `boundary = 80` inside `geom_histogram()` to force ggplot to start a bar at 80 and not include 79.

**Does it look like there's an unexpected jump in the running variable around the cutoff?**

It looks like there potentially could be a jump around the threshold, although the general shape of the distribution is still followed.

```{r}
# Histogram of attendance
ggplot(program, aes(x = attendance, fill = treatment)) +
  geom_histogram(binwidth = 2, colour = "white", boundary = 80) +
  geom_vline(xintercept = 80) +
  labs(x = "School attendance", y = "Count", fill = "In program") +
  theme_classic()
```

Next, conduct a McCrary density test with `rdplotdensity()` from the `rddensity` library.

**Is there a substantial jump at the cutpoint?**

The t-test for the difference in the two points either side of the threshold is very statistically insignificant, and the confidence intervals in the plot (below) overlap substantially. Hence, there's strong evidence that there's no significant difference between the two lines and we can conclude that there's no manipulation of attendance records.

```{r}
# McCrary test
test_density <- rddensity(program$attendance, c = 80)
summary(test_density)

plot_density_test <- rdplotdensity(rdd = test_density,
                                   X = program$attendance,
                                   type = "both")
```

# Step 4: Check for discontinuity in outcome across running variable

Make a scatterplot with the running variable on the x-axis (`attendance`) and the outcome variable on the y-axis (`grade`), with the points colored by treatment (`treatment`). Make the points small and semitransparent since there are a lot of them. Add a vertical line at the cutoff point. Add two `geom_smooth()` lines: one using data before the cutoff and one using data after the cutoff. Make sure both lines use `method = "lm"`.

**Based on this graph, does the program have an effect? Is there a discontinuity in outcome around the cut-point? Interpret the effect (or non-effect) of the program.**

Based on the graph, there could be a very small discontinuity and participation in the attendance program boosted final test grades.

```{r}
# Graph showing discontinuity in grades across levels of attendance
ggplot(program, aes(x = attendance, y = grade, colour = treatment)) +
  geom_point(size = 1.5, alpha = 0.5) +
  geom_smooth(data = filter(program, attendance < 80), method = "lm") +
  geom_smooth(data = filter(program, attendance >= 80), method = "lm") +
  geom_vline(xintercept = 80) +
  labs(x = "School attendance rate", y = "Final test grade", colour = "Used attendance program") +
  theme_classic()
```

# Step 5: Measure the size of the effect

Now you need to measure the size and statistical significance of the discontinuity. If there's a jump because of the program, how big is it and how much can we trust it? You'll do this two ways: (1) parametrically with linear regression and (2) nonparametrically with curvy lines and fancy econometrics algorithms built in to the `rdrobust()` function.

## Parametric estimation

Create a new dataset based on `program` that has a new variable in it named `attendance_centered`. This will be the value of `attendance` minus 80. This centres student attendance around the cut-point (if a student had 85% attendance, they'd have a value of 5; if they had 70% attendance, they'd have a value of 10; etc.) and makes it easier to interpret the intercept coefficient in linear models since it shifts the y-intercept up to the cut-point instead of zero.

```{r}
# Add column to program that centres attendance
program <- program %>% mutate(attendance_centred = attendance - 80)
```

Run a regression model explaining `grade` with `attendance_centered + treatment`:

$$
\text{Grade} = \beta_0 + \beta_1 \text{Attendance (centred)} + \beta_2 \text{Program} + \epsilon
$$

Make sure you use the data frame that has your new `attendance_centered` variable.

**Interpret the three coefficients. How big is the effect of the program? Is it statistically significant?**

```{r}
# Linear model
model_simple <- lm(grade ~ attendance_centred + treatment, data = program)
tidy(model_simple)
```

The coefficients should be interpreted as follows:

- $\beta_0$: This is the intercept. Because we centred attendance rates, it shows the average final test grade at the 80% threshold. Students that had an 80.001% attendance rate scored an average of 66.2 points on their final test. (Alternatively, it shows the predicted final test grade when `attendance_centred` is 0 (i.e. 80) and when `treatment` is `FALSE`).
- $\beta_1$: This is the coefficient for `attendance_centred`. For every percentage point of attendance above 80, students scored 1.56 points higher on the final test.
- $\beta_2$: This is the coefficient for the attendance program and the one most important to us. It is the shift in intercept when `treatment` is TRUE, or the difference between grades at the threshold. Participating in the attendance program increases final test grades by 5.88 points on average.

The final effect is statistically significant, but we shouldn't actually be using the entire dataset to estimate the causal effect as we should only be including students with attendance rates just above and just below the 80% threshold.

Now make two new datasets based on the one you made previously with the `attendance_centered` variable. Filter one so that it only contains observations where `attendance_centered` is between -5 and 5, and filter the other so that it only contains observations where `attendance_centered` is between -10 and 10. 

Run the same model (`grade ~ attendance_centered + program`) using each of these data frames. Interpret the coefficients. Are they different from the model that uses the complete data?

```{r}
# Data and model with bandwidth = 5
model_bw_5 <- lm(grade ~ attendance_centred + treatment,
                 data = filter(program, attendance_centred >= -5 & attendance_centred <= 5))
tidy(model_bw_5)
```

```{r}
# Data and model with bandwidth = 10
model_bw_10 <- lm(grade ~ attendance_centred + treatment,
                  data = filter(program, attendance_centred >= -10 & attendance_centred <= 10))
tidy(model_bw_10)
```

**Put all three models in a side-by-side table with `modelsummary()`. How does the coefficient for `program` change across the model specifications? How does the number of observations change? What advantages and disadvantages are there to restricting the data to ±5 or ±10 around the cut-point? Which program effect do you believe the most? Why?**

```{r}
# All three models
modelsummary(list("Full data" = model_simple,
                  "Bandwidth = 10" = model_bw_10,
                  "Bandwidth = 5" = model_bw_5))
```

The effect of the program differs quite a bit across the different models, from 5.9 to 12.3. I'm inclined to believe either of the restricted models, but definitely not the one using the full dataset.

## Nonparametric estimation

Next you'll use nonparametric estimation to figure out the size of the gap around the cutpoint. Remember from class that this means we're not using straight lines anymore---we're using curvy lines that don't really have neat $y = mx + b$ equations behind them, so we can't use `lm()` and regular coefficients. Instead, we can use `rdrobust` to measure the size of the gap.

Use `rdrobust` with all its default options to find the effect of the program. You'll need to specify `y`, `x`, and `c`. Recall from the in-class example that you'll need to type the name of the variables slighlty differently. To refer to the grade column in the program data frame, you'll have to type `program$grade`. Also, make sure you pipe the output of `rdrobust()` to `summary()`, otherwise you won't see the actual program effect (so make sure you type `rdrobust(...) %>% summary()`).

**How big of an effect does the program have at the cut-point? Is the effect statistically significant?** Important: if you see a negative number, you can pretend that it's positive. It's negative because the change in trend goes down.

The non-parametric approach suggests that the attendance has a 12.0 point effect on final test grades, which is closest to the parametric model with a bandwidth of 10. It's also statistically significant at the 95% confidence level. Note that it chose a similar, but slightly smaller bandwidth of 8.1.

```{r}
# rdrobust()
# Note: You don't need to use attendance_centered anymore; that was just for lm()
rdrobust(y = program$grade, x = program$attendance, c = 80) %>% summary()
```

Make a plot of the effect using `rdplot()`. You'll use the same `y`, `x`, and `c` that you did in `rdrobust()` above. 

```{r}
# Plot
asdf <- rdplot(y = program$grade, x = program$attendance, c = 80)
asdf$rdplot + 
  labs(x = "Attendance rate", y = "Final test grade") +
  theme_classic()
```

## Nonparametric sensitivity checks

Now that we have an effect, we can adjust some of the default options to see how robust the effect size is. 

First we'll play with the bandwidth. Find the ideal bandwidth with with `rdbwselect()`, then run `rdrobust` with twice that bandwidth and half that bandwidth (hint: use `h = SOMETHING`).

```{r}
# Find the ideal bandwidth. Make sure rdbwselect() pipes into summary() so you
# can see the results: rdbwselect() %>% summary()
#
# You'll use the same y, x, and c as before
rdbwselect(y = program$grade, x = program$attendance, c = 80) %>% summary()
```

```{r}
# rdrobust() with half bandwidth
rdrobust(y = program$grade, x = program$attendance, c = 80, h = 8.112/2) %>%
  summary()
# rdrobust() with two times the bandwidth
rdrobust(y = program$grade, x = program$attendance, c = 80, h = 8.112*2) %>%
  summary()
```

Next we'll play with the kernel. Use the default ideal bandwidth and adjust the kernel to change how heavily weighted the observations right by the cutoff are. You already used a triangular kernel---that was the first `rdrobust()` model you ran, since triangular is the default. Try using Epanechnikov and uniform kernels (look at the help file for `rdrobust` or look at the in-class example to see how to specify different kernels):

```{r}
# rdrobust() with an Epanechnikov kernel
rdrobust(y = program$grade, x = program$attendance, c = 80, kernel = "epanechnikov") %>%
  summary()
# rdrobust() with a uniform kernel
rdrobust(y = program$grade, x = program$attendance, c = 80, kernel = "uniform") %>%
  summary()
```


# Step 6: Compare all the effects

**Make a list of all the effects you found. Which one do you trust the most? Why?**

Probably the simplest one - non-parametric with a triangular kernel as KISS is always a good approach, but knowing how much the estimates vary with different specifications is very useful.

Write them in this table if it's helpful:

|     Method    | Bandwidth |        Kernel        | Estimate |
|:-------------:|:---------:|:--------------------:|:--------:|
|   Parametric  | Full data |      Unweighted      |  5.884   |
|   Parametric  |     10    |      Unweighted      |  11.869  |
|   Parametric  |     5     |      Unweighted      |  12.340  |
| Nonparametric |   8.112   |      Triangular      |  12.013  |
| Nonparametric |   4.056   |      Triangular      |  12.761  |
| Nonparametric |  16.224   |      Triangular      |  11.327  |
| Nonparametric |   7.780   |     Epanechnikov     |  12.498  |
| Nonparametric |  16.224   |       Uniform        |  11.081  |


**Does the program have an effect? Should it be rolled out to all schools? Why or why not?**

Probably not. RDD is very much a localised causal effect, so unless you were certain about the similarities of other schools, you couldn't be guaranteed the same results.
