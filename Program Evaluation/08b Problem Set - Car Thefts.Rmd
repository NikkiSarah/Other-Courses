---
title: "Problem set 5: Do police reduce car thefts?"
date: "22 August 2023"
output: 
  html_document: 
    toc: yes
  pdf_document: 
    latex_engine: xelatex
    toc: yes
  word_document: 
    toc: yes
editor_options: 
  chunk_output_type: console
---

---

In 2004, Rafael Di Tella and Ernesto Schargrodsky published a study that analyzed the effect of increased police presence on crime. You looked at this study previously in your threats to validity assignment. To measure this effect, Di Tella and Schargrodsky leveraged a quasi-experiment. Following a synagogue bombing in Buenos Aires, Argentina on July 18, 1994, extra municipal police were assigned to protect synagogues around the city. The increase of police patrols on some city blocks, but not others, means that there is arguably a treatment group and control group for increased police presence, which Di Tella and Schargrodsky used to measure the effect of extra police on car thefts.

The dataset I've provided (`MonthlyPanel.dta`) is a Stata data file nearly 10,000 observations. It comes directly from [Di Tella and Schargrodsky's data appendix available at their study's *AER* webpage](https://www.aeaweb.org/articles?id=10.1257/000282804322970733). This is non-experimental data that includes counts of car thefts for every city block in Buenos Aires from April to December 1994. There are 12 variables:

- `observ` (we'll rename to `block`): The ID number of the block
- `barrio`: The barrio (neighborhood) for the block
- `calle`: The street for the block
- `altura`: The street number
- `institu1` (we'll rename to `same_block`): Indicator variable marking if there's a Jewish institution on the block (1 if yes, 0 if no)
- `institu3`: Indicator variable marking if there's a Jewish institution within one block (1 if yes, 0 if no)
- `distanci` (we'll rename to `distance`): Distance to the nearest Jewish institution, measured in blocks
- `edpub`: Indicator variable marking if there's an educational building or embassy on the block (1 if yes, 0 if no)
- `estserv`: Indicator variable marking if there's a gas station on the block (1 if yes, 0 if no) 
- `banco`: Indicator variable marking if there's a bank on the block (1 if yes, 0 if no) 
- `totrob` (we'll rename to `car_theft`): Total number of car robberies
- `mes` (we'll rename to `month`): Month

---

```{r setup, warning=FALSE, message=FALSE}
install.packages(c("fixest", "modelsummary"), method = "wininet")

library(tidyverse)
library(haven)
library(broom)
library(fixest)        # For fast, nice, fixed effects regression
library(modelsummary)  # For side-by-side regression tables

# This turns off this message that appears whenever you use summarize():
# `summarise()` ungrouping output (override with `.groups` argument)
options(dplyr.summarise.inform = FALSE)

# Load terror data
terror <- read_stata("data/MonthlyPanel.dta") %>% 
  # The attack happened on July 18. The authors omitted data from July 19-31, so
  # all July observations are from before the attack. Make a new indicator
  # variable `after` to mark if the row is from before or after the attack
  mutate(after = mes > 7) %>% 
  # There are some weird months in the data like 73. Filter out anything > 12
  filter(mes <= 12) %>% 
  # Rename some columns to be more readable
  rename(same_block = institu1,
         distance = distanci,
         car_theft = totrob,
         month = mes,
         block = observ) %>% 
  # Create indicator variables for the distance of each block to a synagogue
  mutate(one_block_away = ifelse(distance == 1, 1, 0),
         two_blocks_away = ifelse(distance == 2, 1, 0),
         more_than_two_away = ifelse(distance > 2, 1, 0)) %>% 
  # Make these factors/categories
  mutate(block = as.factor(block),
         month = as.factor(month),
         same_block_factor = as.factor(same_block))
```

# 1. Research design

**Imagine you went out and collected data on the presence of police in each city, and the amount of crime in each city, and found a positive relationship. Does this mean police *cause* crime? Explain.**
No! It could be entirely coincidental that the trends in the police presence and crime are similar. Without accounting for additional variables that could reasonably influence both variables, you can only make statements about the relationship from a correlation perspective.

Di Tella and Ernesto Schargrodsky explore this question with a difference-in-difference design. They collected data on both the presence of police and car robberies in Buenos Aires city blocks both before and after the attack. Their interest is in seeing whether the extra police reduced the amount of car theft. **How is this data suitable for a diff-in-diff design? What would we be comparing here? Be specific about the pre/post treatment/control groups.**

The time factor is taken care of by the before and after attack variable i.e. pre/post treatment. The treatment/control groups would be similar city blocks.

**Why does it help the researchers that the police were dispatched to certain blocks *because of terrorist attacks?***
Because then the police response to the situation should always been comparable. Responses to different situations may be different, which would then possibly be a confounder that would need to be controlled for.

# 2. Trends

One of the most crucial assumptions for difference-in-differences designs is the idea that the trends in the treatment and control groups need to be parallel prior to the intervention or program. **Why?**

In order to reasonably argue that there is nothing going on in either group prior to the intervention/event that may in fact be the underlying driver of the observed difference post-treatment/event rather than the intervention itself.

Create a plot that shows the average number of car thefts per month for blocks with synagogues and blocks without. Add a vertical line (`geom_vline(xintercept = "7")`) in the month where the terror attack happened. 

**What would you say about the parallel trends assumption here? Does it hold up? Maybe? Maybe not?**
I think you would be stretching the truth a little to argue that the parallel trends assumption is reasonable.

```{r plot-trends}
terror %>% 
  group_by(same_block, month) %>% 
  summarise(avg_thefts = mean(car_theft)) %>% 
  ggplot(aes(x = month, y = avg_thefts, group = same_block, colour = as.factor(same_block))) +
  geom_point(size = 3) +
  geom_line(linewidth = 1) +
  geom_vline(xintercept = "7", linetype = "dashed") +
  theme_classic()
```

# 3. Difference-in-differences by hand-ish

Calculate the average number of car thefts in the treatment and control groups before and after the attack. (Hint: group by `same_block` and `after` and find the average of `car_theft`.) 

```{r manual-diff-diff}
# Calculate average of car_theft across same_block and after
thefts_by_year_block <- terror %>% 
  group_by(same_block, after) %>% 
  summarise(avg_thefts = mean(car_theft))

before_treatment <- thefts_by_year_block %>% 
  filter(after == FALSE, same_block == TRUE) %>% 
  pull(avg_thefts)

before_control <- thefts_by_year_block %>% 
  filter(after == FALSE, same_block == FALSE) %>% 
  pull(avg_thefts)

after_treatment <- thefts_by_year_block %>% 
  filter(after == TRUE, same_block == TRUE) %>% 
  pull(avg_thefts)

after_control <- thefts_by_year_block %>% 
  filter(after == TRUE, same_block == FALSE) %>% 
  pull(avg_thefts)

diff_treatment_before_after <- after_treatment - before_treatment
diff_treatment_before_after

diff_control_before_after <- after_control - before_control
diff_control_before_after

diff_diff <- diff_treatment_before_after - diff_control_before_after
diff_diff
```

Calculate the difference-in-difference estimate given these numbers.

|                         | Before attack | After attack | Difference |
|-------------------------|---------------|--------------|------------|
| Block without synagogue | 0.0816        |  0.1047      |  0.0231    |
| Block with synagogue    | 0.0895        |  0.0351      | -0.0544    |
| Difference              | 0.0080        | -0.0696      | -0.0775    |

**How did car thefts change from before-to-after in blocks *without* synagogues?**
In blocks without synagogues, there was a 2.3 percentage point increase in car thefts.
- **How did car thefts change from before-to-after in blocks *with* synagogues?**
In blocks with synagogues, there was a 5.4 percentage point decrease in car thefts.
- **What's the difference-in-differences?**
The diff-in-diff estimate is -0.0775.
- **What does that mean? Interpret the finding.**
This implies that an increased police presence caused a 7.75 percentage point decrease in car thefts (on average).

# 4. Difference-in-differences with regular OLS

Run a regression model to find the diff-in-diff estimate of the effect of the increased police presence (`after`) on car thefts (`car_theft`).

```{r simple-diff-diff-model}
model <- lm(car_theft ~ after + same_block + after*same_block, data = terror)
tidy(model)
```

**How does this value compare with what you found in part 3 earlier? What is the advantage of doing this instead of making a table?**
This is the exact same result. There are two advantages to using a regression model over a 2x2 table: firstly, it's much quicker and significantly reduces the chances of mistakes in the calculations and secondly, you get confidence bands/measures of uncertainty in addition to the point estimate. For example, in this case, we are 95% confident that the causal effect is somewhere in between -2.4 and 13.1 percentage points.

```{r}
confint(model)
```

# 5. Difference-in-differences with fixed effects OLS

The diff-in-diff coefficient you found in part 4 is accurate, but the standard errors and $R^2$ are wrong (run `glance()` on your model object to see how tiny the $R^2$ is)! This is because of a host of mathy reasons, but also because of the DAG. The effect of increased police presence is confounded by both month and block, but all we've really adjusted for binary before/after (for month) and binary synagogue/no synagogue (for block). By reducing these confounders to just binary variables, we lose a lot of the variation across months and blocks.

To fix this, run a diff-in-diff model that includes two additional control variables: `block + month`. 

**Don't use `tidy` to view the results**. You'll get a table with almost 900 rows and it'll take up pages and pages of your knitted document. If you really want to see the results, filter out the block and month rows (like this:).

```r
tidy(name_of_model) %>% 
  filter(!str_starts(term, "month"), 
         !str_starts(term, "block"))
```

```{r fe-model, cache=TRUE}
# Code here with block + month in the model
# DON'T RUN tidy() or modelsummary() without filtering out month and block coefficients
model2 <- lm(car_theft ~ after + same_block + after*same_block + block + month, data = terror)
tidy(model2) %>% 
  filter(!str_starts(term, "month"),
         !str_starts(term, "block"))
```

That slowness is miserable. You can get around that by using a different function for OLS that has built-in support for fixed effects (or indicator variables). The `feols()` (fixed-effects OLS) function from the **fixest** package lets you include indicator variables in regression in a more sophisticated way. The math is lighting fast, and the coefficients for each block and year are hidden by default (though you can still see them if you really want).

The syntax for `feols()` is the same as `lm()`, but with a slight change to accommodate the fixed effects. Use the `|` character to specify a section of the formula that contains the fixed effects: 

```r
model_name <- feols(car_theft ~ same_block*after | block + month, 
                    data = terror)
```

One more cool thing that `feols()` can do that normal `lm()` can't is provide robust standard errors. There is systematic variation within blocks and across time, and we can mathematically account for that variation in the standard errors of the regression. (If you've ever used Stata you do this with `reg y x, robust`). If you ever want to use robust and/or clustered standard errors with regular OLS regression in R, check out the [`lm_robust()` function in the **estimatr** package](https://declaredesign.org/r/estimatr/articles/getting-started.html#lm_robust). With `feols()`, you can add an argument to `tidy()` to get the robust standard errors.

```r
# Stata's default robust SE algorithm is called "Huber-White standard errors", 
# and we can get those same numbers here. Look at the documentation for 
# summary.fixest() for more robustness and clustering options
tidy(model_name, se = "white")
```

Now that you know about `feols()` and robust standard errors, build a model that finds the diff-in-diff effect that includes fixed effects for block and month. Show the results with `tidy()` using Huber-White standard errors.

```{r model-a, message=FALSE}
# Code for model A + use tidy() to show Huber-White robust standard errors
model3 <- feols(car_theft ~ same_block*after | block + month, data = terror)
tidy(model3)
```

In the original study, the authors also considered the effect of two other treatment variables. Maybe the extra police presence in blocks with synagogues reduced car thefts not just for those blocks, but areas 1 block away or 2 blocks away.

Run two more models. In the first, keep the `same_block*after` interaction term and add another diff-in-diff interaction for `one_block_away*after`. In the second, keep the same block and one block interaction terms and add one more diff-in-diff interaction for `two_blocks_away*after`

```{r models-b-c, message=FALSE}
# Code for models B and C + use tidy() to show Huber-White robust standard errors
model4 <- feols(car_theft ~ same_block*after + one_block_away*after | block + month, data = terror)
model5 <- feols(car_theft ~ same_block*after + one_block_away*after + two_blocks_away*after |
                  block + month, data = terror)

tidy(model4, se = "white")
tidy(model5, se = "white")
```

Recreate columns A, B, and C from Table 3 from the original article with `modelsummary()`. You'll need to show the results from your three `feols()` models (with one interaction term, with two interactions, and with three interactions). You can tell the table to show robust standard errors like the authors did in their original study by including the `se = "white"` argument, and you can control how many digits are used with the `fmt` (format) argument (the original article used 5 decimal points, so you can too). You can add significance stars by including `stars = TRUE`. 

```r
modelsummary(list(models, go, here),
             se = "white", fmt = "%.5f", stars = TRUE)
```

```{r show-all-models}
modelsummary(list("Model A" = model3,
                  "Model B" = model4,
                  "Model C" = model5),
             se = "white", fmt = "%.5f", stars = TRUE)
```

**Does having extra police reduce thefts on the same block? Is the effect significant?**
Yes and yes.
**Does having extra police reduce thefts one block away? Is the effect significant?**
Yes, but not significant.
**Does having extra police reduce thefts two blocks away Is the effect significant?**
Yes and again insignificant.

# 6. Translate results to something more interpretable

According to the third model, having additional police on a block caused a reduction of 0.081 car thefts per month on average. What the heck does that even mean though? This whole outcome variable is weird anyway---it's the average number of thefts per block per month, and most block-months have 0 thefts. Having a number like 0.081 doesn't quite represent the proportion of crime or anything logically interpretable or anything. It's a little hard to talk about.

To fix this, we can talk about percent changes instead. Recall from past classes (like microeconomics or GRE prep questions) that you can calculate the percent change (or growth) between two numbers with this formula:

$$
\text{percent change} = \frac{\text{new} - \text{old}}{\text{old}}
$$

You can remember this as **NOO**, for **n**ew minus **o**ld divided by **o**ld. With treatment and outcome groups, you can find the percent change because of a program or policy by using treatment as "new" and outcome as "old".

Imagine if after some program, the treatment group had an outcome of 3 while the control group had an outcome of 6. The percent change in outcome because of the causal effect of the program is $\frac{3 - 6}{6}$, or -0.5:

```{r example-pct-change}
(3 - 6) / 6
```

This means that this fake program *caused* a 50% reduction in the outcome. 

---

Find the percent change in car thefts because of the increase police presence after the July terror attack *using the results from Model C*. To do this, you need two numbers: (1) the average number of thefts in control blocks after the attack, and (2) the average number of thefts in treatment blocks after the attack. Because you're using Model C, your control group includes blocks that don't have synagogues within two blocks.

Use `group_by()` and `summarize()` to calculate the average number of thefts after the attack in control blocks\.

```{r terror-treatment-control}
thefts_by_year_two_blocks <- terror %>% 
  group_by(more_than_two_away, after) %>% 
  summarise(avg_thefts = mean(car_theft))
thefts_by_year_two_blocks
```

Subtract the diff-in-diff effect for "same_block Ã— after" from Model C from the average in the control group to find the average number of car thefts in treatment blocks. (Note: It'll be really tempting to just look at the table for the average for treatment + after, but this won't be right! You need to use control + diff-in-diff, since that's the counterfactual.)

```{r terror-change}
thefts_by_year_two_blocks %>% 
  filter(after == TRUE, more_than_two_away == TRUE) %>% 
  pull(avg_thefts) - model5$coefficients[1]
```

Finally, calculate the percent change in car thefts after the terror attack across treatment and control blocks (hint: the answer is in the third full paragraph on p. 123 of the original article).

```{r terror-pct-change}
# 75%

```
